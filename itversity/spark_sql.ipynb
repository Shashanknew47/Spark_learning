{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12faa465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Windowing Functions'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a9d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\"\"\n",
    "#     Spark sql consists \n",
    "#     - Data Files (Stores data in disks - Datalakes)\n",
    "#     - Meta Data  (Stores data in Metastore - Databases by default Derby)\n",
    "    \n",
    "#     Spark Table (which get generated by Spark Sql) : is a persistant store works like data stores in Databases. \n",
    "#     - Spark table is accessible by another sessions and could be used by another platform like Power Bi or Tableau.\n",
    "    \n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f2c9ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/public/trendytech/retail_db/orders/part-00000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c582ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+---------------+\n",
      "| id|order_date|customer_id|         status|\n",
      "+---+----------+-----------+---------------+\n",
      "|  1|2013-07-25|      11599|         CLOSED|\n",
      "|  2|2013-07-25|        256|PENDING_PAYMENT|\n",
      "|  3|2013-07-25|      12111|       COMPLETE|\n",
      "+---+----------+-----------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = \"id Integer,order_date date, customer_id Integer, status String \"\n",
    "\n",
    "df = spark.read.format('csv').schema(schema).option('path',file_path).load()\n",
    "\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2818049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_path = 'orders'\n",
    "\n",
    "df.write.mode('overwrite')\\\n",
    "    .format('csv')\\\n",
    "    .option('path',dest_path)\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7887a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6b6a9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+---------------+\n",
      "| id|order_date|customer_id|         status|\n",
      "+---+----------+-----------+---------------+\n",
      "|  1|2013-07-25|      11599|         CLOSED|\n",
      "|  2|2013-07-25|        256|PENDING_PAYMENT|\n",
      "|  3|2013-07-25|      12111|       COMPLETE|\n",
      "+---+----------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a  temp table\n",
    "\n",
    "# df.createTempView   (if view is already exists then it will give error)\n",
    "# df.createOrReplaceTempView (if view already exists it will replace old view and will not give any error. )\n",
    "# df.createOrReplaceGlobalTempView (This view will be visible by other applications also)\n",
    "\n",
    "df.createOrReplaceTempView('my_orders')\n",
    "\n",
    "new_df = spark.sql(\"select * from my_orders limit 3 \")\n",
    "\n",
    "new_df.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30152baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+---------------+\n",
      "| id|order_date|customer_id|         status|\n",
      "+---+----------+-----------+---------------+\n",
      "|  1|2013-07-25|      11599|         CLOSED|\n",
      "|  2|2013-07-25|        256|PENDING_PAYMENT|\n",
      "|  3|2013-07-25|      12111|       COMPLETE|\n",
      "|  4|2013-07-25|       8827|         CLOSED|\n",
      "|  5|2013-07-25|      11318|       COMPLETE|\n",
      "|  6|2013-07-25|       7130|       COMPLETE|\n",
      "+---+----------+-----------+---------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can use DataFrame and Sparktable interchangegably\n",
    "\n",
    "spark_table_df = spark.read.table('my_orders')\n",
    "\n",
    "spark_table_df.show(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "752f1207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          namespace|\n",
      "+-------------------+\n",
      "|  0001_av_ivy_tesco|\n",
      "|       003402_hive1|\n",
      "|   005198_ivy_tesco|\n",
      "|   005212_ivy_tesco|\n",
      "|005222_ivy_practice|\n",
      "|005260_ivy_database|\n",
      "|        00ivy_tesco|\n",
      "|         00ivy_test|\n",
      "|      07172021_nyse|\n",
      "|    07172021_retail|\n",
      "|       07172021_sms|\n",
      "|        1230_trendy|\n",
      "|    1230_trendytech|\n",
      "|      1540retail_db|\n",
      "|        1993_ankita|\n",
      "|               1src|\n",
      "|              26may|\n",
      "|               2stg|\n",
      "|               3etl|\n",
      "|           44_tesco|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(spark.sql(\"show databases\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81f020c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-----------+\n",
      "|database|        tableName|isTemporary|\n",
      "+--------+-----------------+-----------+\n",
      "| default|            1htab|      false|\n",
      "| default|   41group_movies|      false|\n",
      "| default|    4group_movies|      false|\n",
      "| default|             4tab|      false|\n",
      "| default|    6_flags_simon|      false|\n",
      "| default|               aa|      false|\n",
      "| default|              abc|      false|\n",
      "| default|             acid|      false|\n",
      "| default|            acid1|      false|\n",
      "| default|     acid_example|      false|\n",
      "| default|    acid_example1|      false|\n",
      "| default|    acid_example2|      false|\n",
      "| default|           adata1|      false|\n",
      "| default|        adata_ell|      false|\n",
      "| default|         adata_vr|      false|\n",
      "| default|    ad_earthquake|      false|\n",
      "| default|ad_earthquake_par|      false|\n",
      "| default|           adelta|      false|\n",
      "| default|       adeltapart|      false|\n",
      "| default|   adeltapartbuck|      false|\n",
      "+--------+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when we have not specified database then it will show default database only.\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1b57fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can create new databases also.\n",
    "\n",
    "spark.sql(\"CREATE DATABASE if not exists itv_006327_retail\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "458150c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|        namespace|\n",
      "+-----------------+\n",
      "|itv_006327_retail|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from  pyspark.sql.functions import col\n",
    "col(\"alphanumeric\").rlike(\"^[0-9]*$\")\n",
    "\n",
    "database_df = spark.sql(\"show databases\").filter(col(\"namespace\").rlike(\"^itv_\\d+27\\w+l$\"))\n",
    "\n",
    "database_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00b47ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To enter into a specific databases;\n",
    "\n",
    "spark.sql('use itv_006327_retail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c2fd45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE if not exists employee (id int, name string)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83085077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>database</th><th>tableName</th><th>isTemporary</th></tr>\n",
       "<tr><td>itv_006327_retail</td><td>employee</td><td>false</td></tr>\n",
       "<tr><td>itv_006327_retail</td><td>itv006327_externa...</td><td>false</td></tr>\n",
       "<tr><td></td><td>my_orders</td><td>true</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+--------------------+-----------+\n",
       "|         database|           tableName|isTemporary|\n",
       "+-----------------+--------------------+-----------+\n",
       "|itv_006327_retail|            employee|      false|\n",
       "|itv_006327_retail|itv006327_externa...|      false|\n",
       "|                 |           my_orders|       true|\n",
       "+-----------------+--------------------+-----------+"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"show tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e0979f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"INSERT INTO itv_006327_retail.employee VALUES  \n",
    "            (1,'Shashank'),\n",
    "            (2,'Rj')\"\"\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57db9eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>name</th></tr>\n",
       "<tr><td>1</td><td>Shashank</td></tr>\n",
       "<tr><td>1</td><td>Shashank</td></tr>\n",
       "<tr><td>1</td><td>Shashank</td></tr>\n",
       "<tr><td>2</td><td>Rj</td></tr>\n",
       "<tr><td>2</td><td>Rj</td></tr>\n",
       "<tr><td>2</td><td>Rj</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+--------+\n",
       "| id|    name|\n",
       "+---+--------+\n",
       "|  1|Shashank|\n",
       "|  1|Shashank|\n",
       "|  1|Shashank|\n",
       "|  2|      Rj|\n",
       "|  2|      Rj|\n",
       "|  2|      Rj|\n",
       "+---+--------+"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('select * FROM employee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2951615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  support you want to load data from a temp table to permanent table\n",
    "#  1 - First create a temp table view with create df.createOrReplaceTempView\n",
    "#  2 - CREate a new table then insert data from temp view. \n",
    "\n",
    "#  Ex.   spark.sql(\"\"\"INSERT INTO itv_006327_retail.orders SELECT * FROM orders \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42a9b3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>col_name</th><th>data_type</th><th>comment</th></tr>\n",
       "<tr><td>id</td><td>int</td><td>null</td></tr>\n",
       "<tr><td>name</td><td>string</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+---------+-------+\n",
       "|col_name|data_type|comment|\n",
       "+--------+---------+-------+\n",
       "|      id|      int|   null|\n",
       "|    name|   string|   null|\n",
       "+--------+---------+-------+"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see details about a table\n",
    "spark.sql(\"describe table itv_006327_retail.employee \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9a2d18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                           |comment|\n",
      "+----------------------------+------------------------------------------------------------------------------------+-------+\n",
      "|id                          |int                                                                                 |null   |\n",
      "|name                        |string                                                                              |null   |\n",
      "|                            |                                                                                    |       |\n",
      "|# Detailed Table Information|                                                                                    |       |\n",
      "|Database                    |itv_006327_retail                                                                   |       |\n",
      "|Table                       |employee                                                                            |       |\n",
      "|Owner                       |itv006327                                                                           |       |\n",
      "|Created Time                |Wed May 10 13:54:48 EDT 2023                                                        |       |\n",
      "|Last Access                 |UNKNOWN                                                                             |       |\n",
      "|Created By                  |Spark 3.0.1                                                                         |       |\n",
      "|Type                        |MANAGED                                                                             |       |\n",
      "|Provider                    |hive                                                                                |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1683894475]                                                  |       |\n",
      "|Statistics                  |48 bytes                                                                            |       |\n",
      "|Location                    |hdfs://m01.itversity.com:9000/user/itv006327/warehouse/itv_006327_retail.db/employee|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                                  |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                                            |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat                          |       |\n",
      "|Storage Properties          |[serialization.format=1]                                                            |       |\n",
      "|Partition Provider          |Catalog                                                                             |       |\n",
      "+----------------------------+------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To see more  details about a table\n",
    "spark.sql(\"describe extended  itv_006327_retail.employee \").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c562330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "  Create a external table \n",
    "  \n",
    "  - When you delete data in external table only meta data gets deleted\n",
    "  - You just specify the location of data along with file formatsap  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(\"\"\" CREATE TABLE itv006327_ext_orders\n",
    "                 (\n",
    "                     order_id Integer,\n",
    "                     order_date string,\n",
    "                     customer_id Integer,\n",
    "                     status String\n",
    "                 )\n",
    "              using csv location 'orders'\n",
    "              \"\"\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb3d3083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------------+-----------+\n",
      "|database         |tableName                |isTemporary|\n",
      "+-----------------+-------------------------+-----------+\n",
      "|itv_006327_retail|employee                 |false      |\n",
      "|itv_006327_retail|itv006327_external_orders|false      |\n",
      "|itv_006327_retail|itv006327_ext_orders     |false      |\n",
      "|                 |my_orders                |true       |\n",
      "+-----------------+-------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51fec40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+---------------+\n",
      "|order_id|order_date|customer_id|         status|\n",
      "+--------+----------+-----------+---------------+\n",
      "|       1|2013-07-25|      11599|         CLOSED|\n",
      "|       2|2013-07-25|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25|      12111|       COMPLETE|\n",
      "+--------+----------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * FROM itv006327_ext_orders  Limit 3\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "457c310d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>col_name</th><th>data_type</th><th>comment</th></tr>\n",
       "<tr><td>order_id</td><td>int</td><td>null</td></tr>\n",
       "<tr><td>order_date</td><td>string</td><td>null</td></tr>\n",
       "<tr><td>customer_id</td><td>int</td><td>null</td></tr>\n",
       "<tr><td>status</td><td>string</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+---------+-------+\n",
       "|   col_name|data_type|comment|\n",
       "+-----------+---------+-------+\n",
       "|   order_id|      int|   null|\n",
       "| order_date|   string|   null|\n",
       "|customer_id|      int|   null|\n",
       "|     status|   string|   null|\n",
       "+-----------+---------+-------+"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql ('describe itv006327_ext_orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a640b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                |comment|\n",
      "+----------------------------+---------------------------------------------------------+-------+\n",
      "|order_id                    |int                                                      |null   |\n",
      "|order_date                  |string                                                   |null   |\n",
      "|customer_id                 |int                                                      |null   |\n",
      "|status                      |string                                                   |null   |\n",
      "|                            |                                                         |       |\n",
      "|# Detailed Table Information|                                                         |       |\n",
      "|Database                    |itv_006327_retail                                        |       |\n",
      "|Table                       |itv006327_ext_orders                                     |       |\n",
      "|Owner                       |itv006327                                                |       |\n",
      "|Created Time                |Fri May 12 08:49:34 EDT 2023                             |       |\n",
      "|Last Access                 |UNKNOWN                                                  |       |\n",
      "|Created By                  |Spark 3.0.1                                              |       |\n",
      "|Type                        |EXTERNAL                                                 |       |\n",
      "|Provider                    |csv                                                      |       |\n",
      "|Location                    |hdfs://m01.itversity.com:9000/user/itv006327/orders      |       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe       |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat         |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat|       |\n",
      "+----------------------------+---------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql ('describe extended itv006327_ext_orders').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea778d87",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Operation not allowed: TRUNCATE TABLE on external tables: `itv_006327_retail`.`itv006327_ext_orders`;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-82ba1db11255>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# you can't run truncate on external tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'truncate table itv006327_ext_orders'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \"\"\"\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Operation not allowed: TRUNCATE TABLE on external tables: `itv_006327_retail`.`itv006327_ext_orders`;"
     ]
    }
   ],
   "source": [
    "# you can't run truncate on external tables\n",
    "\n",
    "spark.sql('truncate table itv006327_ext_orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f9d7677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    You will notice that these new records has inserted in a new file. orginal file remain \n",
    "    \n",
    "    In open source spark we can only do\n",
    "    \n",
    "    - SELECT\n",
    "    - INSERT\n",
    "    \n",
    "    We can't do UPDATE and DELETE which we can do in Databricks Spark.\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(\"\"\" INSERT INTO TABLE itv_006327_retail.itv006327_ext_orders VALUES (11111,'2023-01-01', 2222, 'CLOSED') \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92f02405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/itv006327/${system:java.io.tmpdir}'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96eac9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame_more.ipynb  spark_sql.ipynb  \u001b[0m\u001b[01;34mUntitled Folder\u001b[0m/\n",
      "\u001b[01;34mlearn\u001b[0m/                Untitled1.ipynb\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89336ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
